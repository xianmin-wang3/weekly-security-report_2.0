import requests
import feedparser
import json
import os
import time
from datetime import datetime, timedelta
from bs4 import BeautifulSoup

# è¨­å®š RSS ä¾†æº
RSS_URL = "https://www.nics.nat.gov.tw/RSS2.xml"
URL_FILTER = "nics.nat.gov.tw"

# è¨­å®šæ™‚é–“ç¯„åœï¼ˆéå» 7 å¤©ï¼‰
one_week_ago = datetime.now() - timedelta(days=8)

# æª¢æŸ¥è³‡æ–™å¤¾æ˜¯å¦å­˜åœ¨ï¼Œä¸å­˜åœ¨å‰‡å‰µå»º
output_folder = "../data"
if not os.path.exists(output_folder):
    os.makedirs(output_folder)

# ä½¿ç”¨ requests ä¸‹è¼‰ RSS XMLï¼Œç¦ç”¨ SSL é©—è­‰
print(f"ğŸ” å˜—è©¦ä¸‹è¼‰ RSS: {RSS_URL}")
try:
    response = requests.get(RSS_URL, verify=False)  # ç¦ç”¨ SSL é©—è­‰
    response.raise_for_status()  # æª¢æŸ¥è«‹æ±‚æ˜¯å¦æˆåŠŸ
    print("âœ… RSS XML ä¸‹è¼‰æˆåŠŸ")
except requests.exceptions.RequestException as e:
    print(f"âŒ RSS ä¸‹è¼‰å¤±æ•—: {e}")
    exit()

# å°‡ä¸‹è¼‰çš„ XML å…§å®¹å‚³çµ¦ feedparser è§£æ
feed = feedparser.parse(response.content)

# æª¢æŸ¥ RSS æ˜¯å¦æˆåŠŸè§£æ
if not feed.entries:
    print(f"âŒ RSS è§£æå¤±æ•—ï¼Œæ¢ç›®æ•¸é‡: {len(feed.entries)}")
    print(f"éŒ¯èª¤è¨Šæ¯: {feed.get('bozo_exception', 'ç„¡éŒ¯èª¤è¨Šæ¯')}")
    exit()

print(f"âœ… RSS è§£ææˆåŠŸï¼Œç¸½å…±æœ‰ {len(feed.entries)} ç¯‡æ¢ç›®")
print(f"å˜—è©¦æŠ“å–éå» 7 å¤©ï¼ˆ{one_week_ago.strftime('%Y-%m-%d')} è‡³ä»Šï¼‰çš„æ¼æ´è­¦è¨Šå…¬å‘Šï¼ˆä¾†è‡ª NICSï¼‰\n")

# è¨ˆæ•¸å™¨
article_count = 0

# æŠ“å–éå» 7 å¤©çš„æ¼æ´è­¦è¨Š
for entry in feed.entries:
    if URL_FILTER in entry.link.lower():  # ç¢ºä¿æ˜¯ NICS çš„æ–‡ç« 
        # è§£æç™¼å¸ƒæ™‚é–“
        try:
            if hasattr(entry, 'published_parsed') and entry.published_parsed:
                published_time = datetime.fromtimestamp(time.mktime(entry.published_parsed))
                published_time_str = published_time.strftime('%Y-%m-%d %H:%M:%S')
            else:
                print("   âš ï¸ ç„¡æ³•è§£ææ™‚é–“ï¼Œä½¿ç”¨ç•¶å‰æ™‚é–“ä½œç‚ºå‚™ç”¨")
                published_time = datetime.now()
                published_time_str = published_time.strftime('%Y-%m-%d %H:%M:%S')
        except Exception as e:
            print(f"   âŒ æ™‚é–“è§£æå¤±æ•—: {e}")
            published_time = datetime.now()  # å‚™ç”¨æ™‚é–“
            published_time_str = published_time.strftime('%Y-%m-%d %H:%M:%S')

        # æª¢æŸ¥æ˜¯å¦åœ¨éå» 7 å¤©å…§
        if published_time >= one_week_ago:
            print(f"ğŸ“Œ {entry.title}")
            print(f"   ğŸ•’ ç™¼å¸ƒæ™‚é–“: {published_time_str}")
            print(f"   ğŸ”— é€£çµ: {entry.link}")

            # æå– <description> å…§å®¹ä¸¦ç§»é™¤ HTML æ¨™ç±¤
            raw_content = entry.get('description', '').strip()
            if raw_content:
                # ä½¿ç”¨ BeautifulSoup ç§»é™¤ HTML æ¨™ç±¤
                soup = BeautifulSoup(raw_content, 'html.parser')
                clean_content = soup.get_text(strip=True)  # ç§»é™¤æ¨™ç±¤ï¼Œé€£çºŒæ–‡å­—
                
                # é¡å¤–ç§»é™¤æ–‡å­—ä¸­çš„æ›è¡Œç¬¦
                clean_content = clean_content.replace('\n', '')

                # å„²å­˜è³‡æ–™
                article_data = {
                    "source": "NICS",
                    "title": entry.title,
                    "published_time": published_time_str,
                    "link": entry.link,
                    "content": clean_content  # å„²å­˜ç§»é™¤æ¨™ç±¤ä¸¦èª¿æ•´å¾Œçš„ç´”æ–‡å­—
                }

                filename = f"{output_folder}/{published_time.strftime('%Y%m%d_%H%M%S')}_NICS.json"
                with open(filename, "w", encoding="utf-8") as f:
                    json.dump(article_data, f, ensure_ascii=False, indent=4)

                print(f"   âœ… æ–‡ç« å·²å„²å­˜: {filename}\n")
                article_count += 1
            else:
                print("   âŒ <description> å…§å®¹ç‚ºç©º\n")

print(f"ğŸ‰ å·²å®ŒæˆæŠ“å– {article_count} ç¯‡æ¼æ´è­¦è¨Šï¼ˆéå» 7 å¤©ï¼‰")
