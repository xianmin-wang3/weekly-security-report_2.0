import feedparser
import time
import cloudscraper
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import json
import os

# è¨­å®š RSS ä¾†æº
rss_url = "https://feeds.feedburner.com/TheHackersNews"

# è§£æ RSS
feed = feedparser.parse(rss_url)

# è¨­å®šæ™‚é–“ç¯„åœï¼ˆ7 å¤©å…§ï¼‰
one_week_ago = datetime.now() - timedelta(days=8)

# æª¢æŸ¥è³‡æ–™å¤¾æ˜¯å¦å­˜åœ¨ï¼Œä¸å­˜åœ¨å‰‡å‰µå»º
output_folder = "../data"
if not os.path.exists(output_folder):
    os.makedirs(output_folder)

print(f"ğŸ” å˜—è©¦æŠ“å–éå» 7 å¤©å…§çš„è³‡å®‰æ–°èï¼ˆä¾†è‡ª {feed.feed.title}ï¼‰\n")

# éæ¿¾ 7 å¤©å…§çš„æ–°è
for entry in feed.entries:
    if hasattr(entry, "published_parsed"):  # ç¢ºä¿æœ‰ç™¼ä½ˆæ™‚é–“
        published_time = datetime.fromtimestamp(time.mktime(entry.published_parsed))
        
        # åªå„²å­˜ç¶²å€åŒ…å« "weekly" çš„æ–‡ç« 
        if published_time >= one_week_ago and "weekly" in entry.link.lower():
            print(f"ğŸ“Œ {entry.title}")
            print(f"   ğŸ•’ ç™¼å¸ƒæ™‚é–“: {published_time.strftime('%Y-%m-%d %H:%M:%S')}")
            print(f"   ğŸ”— é€£çµ: {entry.link}")

            # å˜—è©¦æŠ“å–æ–°èå…§æ–‡
            try:
                scraper = cloudscraper.create_scraper()  # ä½¿ç”¨ cloudscraper ç¹é Cloudflare
                response = scraper.get(entry.link)
                response.raise_for_status()  # ç¢ºä¿è«‹æ±‚æˆåŠŸ
                
                soup = BeautifulSoup(response.text, "html.parser")

                # å˜—è©¦æå–ä¸»è¦å…§å®¹
                article_body = soup.find("div", class_="articlebody clear cf")
                if article_body:
                    paragraphs = article_body.find_all("p")  # å–å‡ºæ‰€æœ‰ <p> æ¨™ç±¤
                    content = "\n".join(p.get_text(strip=True) for p in paragraphs)  # é€£æ¥æ®µè½

                    # å°‡å…§æ–‡ä¿å­˜ç‚º JSON æª”æ¡ˆ
                    article_data = {
                        "source": "TheHackersNews",
                        "title": entry.title,
                        "published_time": published_time.strftime('%Y-%m-%d %H:%M:%S'),
                        "link": entry.link,
                        "content": content
                    }

                    # å‰µå»ºæ–‡ç« æ–‡ä»¶ï¼Œä¿å­˜ç‚º JSON
                    filename = f"{output_folder}/{published_time.strftime('%Y%m%d_%H%M%S')}_TheHackersNews.json"
                    with open(filename, "w", encoding="utf-8") as f:
                        json.dump(article_data, f, ensure_ascii=False, indent=4)

                    print(f"   âœ… æ–‡ç« å·²å„²å­˜: {filename}\n")
                    
                    # æ‰¾åˆ°ç¬¦åˆæ¢ä»¶çš„æ–‡ç« å¾Œï¼Œç«‹åˆ»åœæ­¢æŠ“å–
                    break

                else:
                    print("   âŒ ç„¡æ³•æ“·å–å…§æ–‡\n")
            
            except Exception as e:
                print(f"   âŒ ä¸‹è¼‰å¤±æ•—: {e}\n")
